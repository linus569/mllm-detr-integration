{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/linus/Developer/master-thesis/master-thesis/.env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please install pyav to use video processing functions.\n",
      "OpenCLIP not installed\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from llava.mm_utils import process_images\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Add the src directory to the Python path\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\", \"src\")))\n",
    "\n",
    "from utils.train_utils import build_dataloader, build_train_dataloader, build_val_dataloader\n",
    "from dataset.processor import Processor\n",
    "from model.model import VisionLanguageModel\n",
    "from model.fastrcnn_adapter import FastRCNNAdapter\n",
    "from utils.config import DatasetConfig, ExperimentConfig\n",
    "from utils.train_metrics import TrainMetrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hydra imports\n",
    "import os\n",
    "from hydra import initialize, initialize_config_module, initialize_config_dir, compose\n",
    "from omegaconf import OmegaConf\n",
    "from hydra.core.config_store import ConfigStore\n",
    "\n",
    "OmegaConf.register_new_resolver(\n",
    "    \"ifel\", lambda flag, val_true, val_false: val_true if flag else val_false\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: local_test\n",
      "seed: 43\n",
      "train_dataset:\n",
      "  name: coco_train\n",
      "  data_dir: ${main_dir}/data/coco/images/train2017\n",
      "  annotations_dir: ${main_dir}/data/coco/annotations/instances_train2017.json\n",
      "val_dataset:\n",
      "  name: coco_val\n",
      "  data_dir: ${main_dir}/data/coco/images/val2017\n",
      "  annotations_dir: ${main_dir}/data/coco/annotations/instances_val2017.json\n",
      "test_dataset:\n",
      "  name: coco_test\n",
      "  data_dir: ${main_dir}/data/coco/images/test2017\n",
      "  annotations_dir: ${main_dir}/data/coco/annotations/image_info_test2017.json\n",
      "main_dir: ..\n",
      "model_name: lmms-lab/llava-onevision-qwen2-0.5b-si\n",
      "checkpoint_dir: checkpoints\n",
      "load_checkpoint: null\n",
      "train: true\n",
      "evaluate: true\n",
      "eval_mode: val\n",
      "num_samples: 15\n",
      "val_num_samples: 2\n",
      "max_tokens: 3200\n",
      "pad_to_multiple_of: 128\n",
      "batch_size: 1\n",
      "total_batch_size: 2\n",
      "epochs: 2\n",
      "lr: 0.001\n",
      "warmup_ratio: 0.1\n",
      "weight_decay: null\n",
      "max_grad_norm: null\n",
      "val_freq: 5\n",
      "val_ep: null\n",
      "print_freq: 1\n",
      "num_workers: 0\n",
      "device: cpu\n",
      "debug: true\n",
      "save_components: []\n",
      "temperature: 0.3\n",
      "use_amp: false\n",
      "torch_dtype: null\n",
      "image_size:\n",
      "- 384\n",
      "- 384\n",
      "num_image_tokens: 729\n",
      "image_token: <image>\n",
      "num_coordinate_bins: 100\n",
      "add_special_tokens: true\n",
      "freeze_model: true\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load hydra configs\n",
    "cs = ConfigStore.instance()\n",
    "cs.store(name=\"ExperimentConfig\", node=ExperimentConfig)\n",
    "cs.store(name=\"DatasetConfig\", group=\"dataset\", node=DatasetConfig)\n",
    "# OmegaConf.register_new_resolver(\"models_dir\", lambda: MODELS_DIR)\n",
    "\n",
    "\n",
    "with initialize(version_base=None, config_path=\"../conf\"):\n",
    "    config = compose(config_name=\"train\", overrides=[\"+experiment=train_local_test\", \"main_dir='..'\"])\n",
    "    print(OmegaConf.to_yaml(config))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load processor, tokenizer, val_dataloader, batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"last_model_silver-field-126.pt\" #\"checkpoint_1_vital-sound-133_1741647312.pt\" #\"last_model_legendary-cloud-125.pt\"\n",
    "MODEL_NAME = \"checkpoint_3_rare-fire-135_1741767317.pt\" #\"checkpoint_3_balmy-snow-134_1741766686.pt\"\n",
    "config.num_coordinate_bins = 100\n",
    "config.add_special_tokens = True # False\n",
    "\n",
    "processor = Processor.from_config(config, add_special_tokens=config.add_special_tokens)\n",
    "tokenizer = processor.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.16s)\n",
      "creating index...\n",
      "index created!\n",
      "torch.Size([0])\n",
      "\n",
      "All labels are -100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['<annotation><object><class>car</class><bbox><x14/><y36/><x18/><y43/></bbox></object><object><class>surfboard</class><bbox><x00/><y40/><x85/><y67/></bbox></object></annotation>']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_dataloader = build_val_dataloader(config=config, processor=processor, subset_size=10)\n",
    "val_batch = next(iter(val_dataloader))\n",
    "\n",
    "# test labels for train dataset\n",
    "labels = val_batch[\"labels\"][0][val_batch[\"labels\"][0] != -100]\n",
    "print(labels.shape)\n",
    "print(tokenizer.decode(labels))\n",
    "\n",
    "#check if labels is just -100\n",
    "if torch.all(val_batch[\"labels\"] == -100):\n",
    "    print(\"All labels are -100\")\n",
    "\n",
    "val_batch[\"bbox_str\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=4.00s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "train_dataloader = build_train_dataloader(config=config, processor=processor, subset_size=10)\n",
    "train_batch = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading vision tower: google/siglip-so400m-patch14-384\n",
      "FasterRCNNWithLLaVA(\n",
      "  (backbone): LLaVAFeatureExtractor(\n",
      "    (llava_encoder): SigLipVisionTower(\n",
      "      (vision_tower): SigLipVisionModel(\n",
      "        (vision_model): SigLipVisionTransformer(\n",
      "          (embeddings): SigLipVisionEmbeddings(\n",
      "            (patch_embedding): Conv2d(3, 1152, kernel_size=(14, 14), stride=(14, 14), padding=valid)\n",
      "            (position_embedding): Embedding(729, 1152)\n",
      "          )\n",
      "          (encoder): SigLipEncoder(\n",
      "            (layers): ModuleList(\n",
      "              (0-25): 26 x SigLipEncoderLayer(\n",
      "                (self_attn): SigLipAttention(\n",
      "                  (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "                  (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "                  (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "                  (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "                )\n",
      "                (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "                (mlp): SigLipMLP(\n",
      "                  (activation_fn): PytorchGELUTanh()\n",
      "                  (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
      "                  (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
      "                )\n",
      "                (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (post_layernorm): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "          (head): Identity()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (projection): Conv2d(1152, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "  )\n",
      "  (model): FasterRCNN(\n",
      "    (transform): GeneralizedRCNNTransform(\n",
      "        Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      "        Resize(min_size=(800,), max_size=1333, mode='bilinear')\n",
      "    )\n",
      "    (backbone): LLaVAFeatureExtractor(\n",
      "      (llava_encoder): SigLipVisionTower(\n",
      "        (vision_tower): SigLipVisionModel(\n",
      "          (vision_model): SigLipVisionTransformer(\n",
      "            (embeddings): SigLipVisionEmbeddings(\n",
      "              (patch_embedding): Conv2d(3, 1152, kernel_size=(14, 14), stride=(14, 14), padding=valid)\n",
      "              (position_embedding): Embedding(729, 1152)\n",
      "            )\n",
      "            (encoder): SigLipEncoder(\n",
      "              (layers): ModuleList(\n",
      "                (0-25): 26 x SigLipEncoderLayer(\n",
      "                  (self_attn): SigLipAttention(\n",
      "                    (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "                    (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "                    (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "                    (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "                  )\n",
      "                  (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "                  (mlp): SigLipMLP(\n",
      "                    (activation_fn): PytorchGELUTanh()\n",
      "                    (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
      "                    (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
      "                  )\n",
      "                  (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (post_layernorm): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "            (head): Identity()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (projection): Conv2d(1152, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "    (rpn): RegionProposalNetwork(\n",
      "      (anchor_generator): AnchorGenerator()\n",
      "      (head): RPNHead(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "        )\n",
      "        (cls_logits): Conv2d(256, 15, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (bbox_pred): Conv2d(256, 60, kernel_size=(1, 1), stride=(1, 1))\n",
      "      )\n",
      "    )\n",
      "    (roi_heads): RoIHeads(\n",
      "      (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0'], output_size=(7, 7), sampling_ratio=2)\n",
      "      (box_head): TwoMLPHead(\n",
      "        (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n",
      "        (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      )\n",
      "      (box_predictor): FastRCNNPredictor(\n",
      "        (cls_score): Linear(in_features=1024, out_features=91, bias=True)\n",
      "        (bbox_pred): Linear(in_features=1024, out_features=364, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = FastRCNNAdapter(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 384, 384])\n",
      "{'map': 0.0, 'map_50': 0.0, 'map_75': 0.0, 'bleu_score': 0, 'meteor_score': 0}\n"
     ]
    }
   ],
   "source": [
    "# test model generate\n",
    "input_ids = train_batch[\"input_ids\"]\n",
    "images = train_batch[\"images\"]\n",
    "\n",
    "print(images.shape)\n",
    "\n",
    "output = model.generate(input_ids=input_ids, image=images)\n",
    "#print(output)\n",
    "\n",
    "target_boxes = processor.postprocess_target_batch(batch=train_batch, device=config.device)\n",
    "#print(target_boxes)\n",
    "\n",
    "metric = TrainMetrics(config.device, download_nltk=False)\n",
    "metric.update(output, target_boxes, None, None)\n",
    "print(metric.compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading vision tower: google/siglip-so400m-patch14-384\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.6366, -0.7641, -0.2013,  ..., -1.2936,  2.3574, -3.9616],\n",
       "         [-1.9123, -2.6315, -0.3732,  ..., -2.0648, -0.5618,  0.7966],\n",
       "         [-1.7799, -2.5519, -0.2654,  ..., -1.9276, -0.8101,  0.4554],\n",
       "         ...,\n",
       "         [-0.6656, -1.5752, -1.7274,  ...,  1.2125, -0.9415,  0.0773],\n",
       "         [-1.0145, -4.4144,  0.3728,  ..., -0.7377,  0.8733,  2.7178],\n",
       "         [ 0.0412,  0.3303,  1.5648,  ..., -3.3311,  2.6198, -0.7386]]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llava.model.language_model.llava_qwen import LlavaQwenForCausalLM\n",
    "\n",
    "image_encoder = LlavaQwenForCausalLM.from_pretrained(\n",
    "    \"lmms-lab/llava-onevision-qwen2-0.5b-si\"\n",
    ").get_vision_tower()\n",
    "\n",
    "image_encoder(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CausalLMOutputWithPast(loss=tensor(5.2896, grad_fn=<AddBackward0>), logits=None, past_key_values=None, hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "# test model forward\n",
    "output = model(input_ids=input_ids, images=images, labels=target_boxes)\n",
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
