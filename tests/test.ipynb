{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from llava.mm_utils import process_images\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Add the src directory to the Python path\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\", \"src\")))\n",
    "\n",
    "from dataset.dataset import DatasetConfig, build_dataloader\n",
    "from dataset.processor import Processor\n",
    "from model.model import VisionLanguageModel\n",
    "from utils.train_utils import ExperimentConfig\n",
    "\n",
    "\n",
    "import os\n",
    "from hydra import initialize, initialize_config_module, initialize_config_dir, compose\n",
    "from omegaconf import OmegaConf\n",
    "from hydra.core.config_store import ConfigStore\n",
    "\n",
    "OmegaConf.register_new_resolver(\n",
    "    \"ifel\", lambda flag, val_true, val_false: val_true if flag else val_false\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load hydra configs\n",
    "cs = ConfigStore.instance()\n",
    "cs.store(name=\"ExperimentConfig\", node=ExperimentConfig)\n",
    "cs.store(name=\"DatasetConfig\", group=\"dataset\", node=DatasetConfig)\n",
    "# OmegaConf.register_new_resolver(\"models_dir\", lambda: MODELS_DIR)\n",
    "\n",
    "\n",
    "with initialize(version_base=None, config_path=\"../conf\"):\n",
    "    config = compose(config_name=\"train\", overrides=[\"+experiment=train_local_test\", \"main_dir='..'\"])\n",
    "    print(OmegaConf.to_yaml(config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load example from coco dataset\n",
    "train_data_dir = \"../data/coco/images/train2017\"\n",
    "train_annotation_file = \"../data/coco/annotations/instances_train2017.json\"\n",
    "model_name = \"lmms-lab/llava-onevision-qwen2-0.5b-si\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "num_img_tokens = 729\n",
    "\n",
    "dataloader = build_dataloader(\n",
    "    config=config,\n",
    "    dataset_config=config.train_dataset,\n",
    "    batch_size=2,#config.batch_size,\n",
    "    is_train=False, # val_dataset\n",
    "    num_workers=config.num_workers,\n",
    "    # image_size=config.transform.image_size,\n",
    "    num_image_tokens=num_img_tokens,\n",
    "    subset_size=10,\n",
    "    # use_random_subset=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(dataloader))\n",
    "\n",
    "# test labels for train dataset\n",
    "# labels = batch[\"labels\"][batch[\"labels\"] != -100]\n",
    "# print(tokenizer.decode(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.data_utils import show_img_with_bbox\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "\n",
    "dataset = dataloader.dataset\n",
    "# Get original dataset from dataset if Subset is used\n",
    "if hasattr(dataset, \"dataset\"):\n",
    "    dataset = dataset.dataset\n",
    "\n",
    "class_id_to_name = dataset.index_to_cat_name\n",
    "\n",
    "#print(example_batch)\n",
    "\n",
    "#fig, ax = plt.subplots()   \n",
    "axes = show_img_with_bbox(batch, dataset, figsize=(10,10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device=torch.device(\"cpu\")\n",
    "\n",
    "model = VisionLanguageModel(model_name, config).to(device)\n",
    "# checkpoint_0_1740259806.pt\n",
    "# checkpoint_0_1740425842.pt\n",
    "state_dict = torch.load(\"../../checkpoints-trained/last_model_hardy-energy-102.pt\", map_location=device)\n",
    "model.projector.load_state_dict(state_dict.get(\"model_state_dict\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with np.printoptions(threshold=np.inf):\n",
    "    #print(batch[\"input_ids\"][0].numpy())\n",
    "    print(tokenizer.decode(batch[\"input_ids\"][0].numpy()))\n",
    "    #print(batch[\"attention_mask\"][0].numpy())\n",
    "\n",
    "tokenizer.batch_decode(batch[\"input_ids\"], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Model with pretrained projection layer\n",
    "from utils.train_utils import JSONStoppingCriteria, parse_model_output_to_boxes\n",
    "\n",
    "model.eval()\n",
    "# TODO: use val set, so info bout bbox is not in input_ids, check if image tokens are filled with image info\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids=batch[\"input_ids\"].to(device),\n",
    "    attention_mask=batch[\"attention_mask\"].to(device),\n",
    "    image=batch[\"images\"].to(device),\n",
    "    stopping_criteria=[JSONStoppingCriteria(model.tokenizer)],\n",
    "    do_sample=True,\n",
    "    temperature=.6,\n",
    "    top_p = 0.9,\n",
    "    top_k = 100,\n",
    "    \n",
    "    #max_new_tokens=1000,\n",
    ")\n",
    "\n",
    "# Decode predictions\n",
    "generated_text = model.tokenizer.batch_decode(\n",
    "    outputs, skip_special_tokens=False\n",
    ")\n",
    "print(generated_text)\n",
    "predicted_boxes = parse_model_output_to_boxes(generated_text, dataset, device)\n",
    "    \n",
    "\n",
    "predicted_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predicted boxes, target boxes and labels on images\n",
    "id_to_cat_name = dataset.index_to_cat_name\n",
    "print(predicted_boxes)\n",
    "\n",
    "#predicted_boxes = [{\"class\": [1, 32], \"bbox\": [[0.4879453125, 0.6142578125, 0.6474609375, 0.814453125], [0.0, 0.0, 0.99951171875, 0.9990234375]]}]\n",
    "\n",
    "for i in range(len(batch[\"images\"])):\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    img, bboxes, categories = batch[\"images\"][i], predicted_boxes[i][\"boxes\"], predicted_boxes[i][\"labels\"]\n",
    "\n",
    "    img = img.permute(1, 2, 0).numpy()\n",
    "    img = img - img.min()\n",
    "    img = img / img.max()\n",
    "    ax.imshow(img)\n",
    "\n",
    "    for cat, bbox in zip(categories, bboxes):\n",
    "        # print(bbox)\n",
    "        x1, y1, x2, y2 = bbox # x_min, y_min, x_max, y_max -> YOLO format\n",
    "        # x1, y1, x2, y2 = x1*img.shape[1], y1*img.shape[0], x2*img.shape[1], y2*img.shape[0] \n",
    "        rect = plt.Rectangle((x1, y1), x2-x1, y2-y1, linewidth=1, edgecolor=\"r\", facecolor=\"none\")\n",
    "        ax.add_patch(rect)\n",
    "        \n",
    "        # add label text to rect\n",
    "        if cat.item() in id_to_cat_name:\n",
    "            class_name = id_to_cat_name[cat.item()] #no .item()\n",
    "        else:\n",
    "            class_name = \"Unknown\"\n",
    "        ax.text(x1, y1-5, class_name, fontsize=12, color=\"red\")\n",
    "\n",
    "    corr_boxes, corr_labels = batch[\"instance_bboxes\"][i], batch[\"instance_classes_id\"][i]\n",
    "\n",
    "    for cat, bbox in zip(corr_labels, corr_boxes):\n",
    "        x1, y1, x2, y2 = bbox\n",
    "        x1, y1, x2, y2 = (\n",
    "                x1 * img.shape[1],\n",
    "                y1 * img.shape[0],\n",
    "                x2 * img.shape[1],\n",
    "                y2 * img.shape[0],\n",
    "            )\n",
    "        rect = plt.Rectangle((x1, y1), x2-x1, y2-y1, linewidth=1, edgecolor=\"g\", facecolor=\"none\")\n",
    "        ax.add_patch(rect)\n",
    "        \n",
    "        # add label text to rect\n",
    "        if cat.item() in id_to_cat_name:\n",
    "            class_name = id_to_cat_name[cat.item()]\n",
    "        ax.text(x1, y1-5, class_name, fontsize=12, color=\"green\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Train Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate test metrics\n",
    "from utils.train_metrics import TrainMetrics\n",
    "from utils.train_utils import unnormalize_bbox\n",
    "device = \"cpu\"\n",
    "metrics = TrainMetrics(device=device)\n",
    "\n",
    "target_boxes = [\n",
    "    {\n",
    "        \"boxes\": unnormalize_bbox(\n",
    "            boxes.to(device),\n",
    "            model.image_size,\n",
    "            model.image_size,\n",
    "        ),\n",
    "        \"labels\": labels.to(device),\n",
    "    }\n",
    "    for boxes, labels in zip(\n",
    "        batch[\"instance_bboxes\"], batch[\"instance_classes_id\"]\n",
    "    )\n",
    "]\n",
    "\n",
    "metrics.update(\n",
    "    predicted_boxes=predicted_boxes,\n",
    "    target_boxes=target_boxes,\n",
    "    target_texts=batch[\"bbox_str\"],\n",
    "    generated_text=generated_text,\n",
    ")\n",
    "metrics.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_boxes_test = [{\n",
    "        'boxes': torch.tensor([[183.7680,  92.0112, 332.6478, 226.5556], [169.6680,   5.6480, 324.7800, 377.7072]]), \n",
    "        'labels': torch.tensor([17, 70]), \n",
    "        'scores': torch.tensor([1., 1.])\n",
    "    }, \n",
    "    {\n",
    "        'boxes': torch.tensor([[305.1480, 223.1204, 320.6880, 235.3108], [ 73.9140,  37.1321, 219.9680, 262.9113]]),\n",
    "        'labels': torch.tensor([15,  1]), \n",
    "        'scores': torch.tensor([1., 1.])\n",
    "    }]\n",
    "target_boxes_test = [{\n",
    "        'boxes': torch.tensor([[169.6680,   5.6480, 324.7800, 377.7072], [225.7680,  92.0112, 332.6478, 226.5556]]), \n",
    "        'labels': torch.tensor([70, 17]), \n",
    "        'scores': torch.tensor([1., 1.])\n",
    "    }, \n",
    "    {\n",
    "        'boxes': torch.tensor([[305.1480, 223.1204, 320.6880, 235.3108], [ 73.9140,  37.1321, 219.9680, 262.9113]]),\n",
    "        'labels': torch.tensor([15,  1]), \n",
    "        'scores': torch.tensor([1., 1.])\n",
    "    }]\n",
    "\n",
    "test_metrics = TrainMetrics(device=device)\n",
    "test_metrics.update(predicted_boxes=pred_boxes_test, target_boxes=target_boxes_test, target_texts=batch[\"bbox_str\"], generated_text=generated_text)\n",
    "test_metrics.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Else"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.train_utils import build_train_dataloader\n",
    "\n",
    "dataloader = build_train_dataloader(config, model)\n",
    "len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "\n",
    "if device == torch.device(\"cuda\"):\n",
    "    print(\"Using CUDA\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model to bfloat16\n",
    "model = model.to(torch.bfloat16)\n",
    "\n",
    "model.generate(\n",
    "    input_ids=batch[\"input_ids\"].to(device),\n",
    "    attention_mask=batch[\"attention_mask\"].to(device),\n",
    "    image=batch[\"images\"].to(device, torch.bfloat16),\n",
    "    stopping_criteria=[JSONStoppingCriteria(model.tokenizer)],\n",
    "    do_sample=True,\n",
    "    temperature=0.3,\n",
    "    top_p = 0.9,\n",
    "    top_k = 50,\n",
    "    #max_new_tokens=1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.train_utils import build_train_dataloader\n",
    "train_dl = build_train_dataloader(config, model)\n",
    "\n",
    "batch = next(iter(train_dl))\n",
    "\n",
    "model.tokenizer.batch_decode(batch[\"input_ids\"], skip_special_tokens=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
