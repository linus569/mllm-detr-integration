{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from llava.mm_utils import process_images\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Add the src directory to the Python path\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\", \"src\")))\n",
    "\n",
    "from utils.train_utils import build_dataloader\n",
    "from dataset.processor import Processor\n",
    "from model.model import VisionLanguageModel\n",
    "from utils.config import DatasetConfig, ExperimentConfig\n",
    "\n",
    "\n",
    "import os\n",
    "from hydra import initialize, initialize_config_module, initialize_config_dir, compose\n",
    "from omegaconf import OmegaConf\n",
    "from hydra.core.config_store import ConfigStore\n",
    "\n",
    "OmegaConf.register_new_resolver(\n",
    "    \"ifel\", lambda flag, val_true, val_false: val_true if flag else val_false\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load hydra configs\n",
    "cs = ConfigStore.instance()\n",
    "cs.store(name=\"ExperimentConfig\", node=ExperimentConfig)\n",
    "cs.store(name=\"DatasetConfig\", group=\"dataset\", node=DatasetConfig)\n",
    "# OmegaConf.register_new_resolver(\"models_dir\", lambda: MODELS_DIR)\n",
    "\n",
    "\n",
    "with initialize(version_base=None, config_path=\"../conf\"):\n",
    "    config = compose(config_name=\"train\", overrides=[\"+experiment=train_local_test\", \"main_dir='..'\"])\n",
    "    print(OmegaConf.to_yaml(config))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load processor, tokenizer, val_dataloader, batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"last_model_silver-field-126.pt\" #\"checkpoint_1_vital-sound-133_1741647312.pt\" #\"last_model_legendary-cloud-125.pt\"\n",
    "MODEL_NAME = \"checkpoint_3_rare-fire-135_1741767317.pt\" #\"checkpoint_3_balmy-snow-134_1741766686.pt\"\n",
    "config.num_coordinate_bins = 100\n",
    "config.add_special_tokens = True # False\n",
    "\n",
    "processor = Processor.from_config(config, add_special_tokens=config.add_special_tokens)\n",
    "tokenizer = processor.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataloader = build_dataloader(\n",
    "    processor=processor,\n",
    "    dataset_config=config.train_dataset,\n",
    "    batch_size=2,#config.batch_size,\n",
    "    is_train=False, # val_dataset # CURRENTLY TRUE\n",
    "    num_workers=config.num_workers,\n",
    "    subset_size=10,\n",
    "    # use_random_subset=True,\n",
    ")\n",
    "\n",
    "val_batch = next(iter(val_dataloader))\n",
    "\n",
    "# test labels for train dataset\n",
    "labels = val_batch[\"labels\"][0][val_batch[\"labels\"][0] != -100]\n",
    "print(labels.shape)\n",
    "print(tokenizer.decode(labels))\n",
    "\n",
    "#check if labels is just -100\n",
    "if torch.all(val_batch[\"labels\"] == -100):\n",
    "    print(\"All labels are -100\")\n",
    "\n",
    "val_batch[\"labels\"], val_batch[\"bbox_str\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.data_utils import show_img_with_bbox\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "\n",
    "dataset = val_dataloader.dataset\n",
    "# Get original dataset from dataset if Subset is used\n",
    "if hasattr(dataset, \"dataset\"):\n",
    "    dataset = dataset.dataset\n",
    "\n",
    "class_id_to_name = dataset.index_to_cat_name\n",
    "\n",
    "#print(example_batch)\n",
    "\n",
    "#fig, ax = plt.subplots()   \n",
    "axes = show_img_with_bbox(val_batch, dataset, figsize=(10,10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pretrained model and check batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.model import VisionLanguageModel\n",
    "model = VisionLanguageModel(\n",
    "    config=config,\n",
    "    image_token_index=processor.image_token_index,\n",
    "    num_new_tokens=len(processor.special_tokens),\n",
    "    do_init=config.add_special_tokens,\n",
    "    initializers=processor.special_tokens_initializer\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device=torch.device(\"cpu\")\n",
    "state_dict = torch.load(\"../../checkpoints-trained/\" + MODEL_NAME, map_location=device)\n",
    "model.load_state_dict(state_dict.get(\"model_state_dict\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check batch \n",
    " - decode val prompt\n",
    " - check tokenized special tokens\n",
    " - check format input bbox to xml\n",
    " - print decoded input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decoded validation prompt\n",
    "processor.tokenizer.batch_decode(val_batch[\"input_ids\"], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if special tokens is correclty encoded, if add_special_tokens is True\n",
    "str_special_tokens = \"<object><x34/>\"\n",
    "tokenizer.encode(str_special_tokens, add_special_tokens=False), config.add_special_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check how model formats the input to xml\n",
    "classes_str = [\"class1\", \"class2\"]\n",
    "normalized_bbox = [[0.08, 0.1, 0.9, 0.9], [0.11, 0.4, 0.6, 0.7]]\n",
    "processor.format_bbox_to_xml(classes_str, normalized_bbox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with np.printoptions(threshold=np.inf):\n",
    "    print(tokenizer.decode(val_batch[\"input_ids\"][0].numpy()))\n",
    "    #print(batch[\"attention_mask\"][0].numpy())\n",
    "\n",
    "tokenizer.batch_decode(val_batch[\"input_ids\"], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt1 = \"Detect all objects in the image and output ONLY a valid JSON array of objects. Each object must have a 'class' (string name) and 'bbox' (normalized coordinates [x_min, y_min, x_max, y_max] between 0 and 1). Format: [{'class': 'person', 'bbox': [0.2, 0.3, 0.5, 0.8]}, {'class': 'car', 'bbox': [0.6, 0.7, 0.9, 0.95]}]. Include all visible objects, even if partially visible. Output nothing but the JSON array.\"\n",
    "prompt2 = \"Detect all objects in the image and output ONLY a valid JSON array of objects. Each object must have a 'class' (string name) and 'bbox' (list of 4 special coordinate tokens describing [x_min, y_min, x_max, y_max]). Format: [{'class': 'person', 'bbox': ['<coord_2>', '<coord_3>', '<coord_5>', '<coord_8>']}, {'class': 'car', 'bbox': ['<coord_6>', '<coord_7>', '<coord_9>', '<coord_9>']}]. Each <coord_X> token represents a quantized position. Include all visible objects, even if partially visible. Output nothing but the JSON array.\"\n",
    "\n",
    "example_xml = \"<annotation><object><class>car</class><bbox x0='0.14673' y0='0.36377' x1='0.18527' y1='0.44438'/></object><object><class>surfboard</class><bbox x0='0.0' y0='0.41329' x1='0.86317' y1='0.67906'/></object></annotation>\"\n",
    "prompt3 = f\"Detect all objects in the image and output ONLY a valid XML of list of object. Each <object> must have a <class> (string name) and <bbox> (list of 4 special coordinate tokens <x0><y0><x1><y1>). Format: {example_xml}. Include all visible objects, even if partially visible. Output nothing but the XML.\"\n",
    "\n",
    "# len of new prompt 2 with special tokens shorter than prompt 1, even tough string is longer\n",
    "len(tokenizer.encode(prompt1)), len(tokenizer.encode(prompt2)), len(tokenizer.encode(prompt3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate output with pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Model with pretrained projection layer\n",
    "from utils.train_utils import JSONStoppingCriteria\n",
    "model.eval()\n",
    "# TODO: use val set, so info bout bbox is not in input_ids, check if image tokens are filled with image info\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids=val_batch[\"input_ids\"].to(device),\n",
    "    attention_mask=val_batch[\"attention_mask\"].to(device),\n",
    "    image=val_batch[\"images\"].to(device),\n",
    "    stopping_criteria=[JSONStoppingCriteria(processor.tokenizer)],\n",
    "    do_sample=True,\n",
    "    temperature=.8,\n",
    "    top_p = 0.9,\n",
    "    top_k = 50,\n",
    ")\n",
    "\n",
    "# Decode predictions\n",
    "generated_text, predicted_boxes = processor.postprocess_xml_batch(outputs, dataset, device)\n",
    "print(len(outputs[0]), len(outputs[1]))\n",
    "\n",
    "predicted_boxes, generated_text, val_batch[\"bbox_str\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predicted boxes, target boxes and labels on images\n",
    "id_to_cat_name = dataset.index_to_cat_name\n",
    "print(predicted_boxes)\n",
    "\n",
    "#predicted_boxes = [{\"class\": [1, 32], \"bbox\": [[0.4879453125, 0.6142578125, 0.6474609375, 0.814453125], [0.0, 0.0, 0.99951171875, 0.9990234375]]}]\n",
    "\n",
    "for i in range(len(val_batch[\"images\"])):\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    img, bboxes, categories = val_batch[\"images\"][i], predicted_boxes[i][\"boxes\"], predicted_boxes[i][\"labels\"]\n",
    "\n",
    "    img = img.permute(1, 2, 0).numpy()\n",
    "    img = img - img.min()\n",
    "    img = img / img.max()\n",
    "    ax.imshow(img)\n",
    "\n",
    "    for cat, bbox in zip(categories, bboxes):\n",
    "        # print(bbox)\n",
    "        x1, y1, x2, y2 = bbox # x_min, y_min, x_max, y_max -> YOLO format\n",
    "        # x1, y1, x2, y2 = x1*img.shape[1], y1*img.shape[0], x2*img.shape[1], y2*img.shape[0] \n",
    "        rect = plt.Rectangle((x1, y1), x2-x1, y2-y1, linewidth=1, edgecolor=\"r\", facecolor=\"none\")\n",
    "        ax.add_patch(rect)\n",
    "        \n",
    "        # add label text to rect\n",
    "        if cat.item() in id_to_cat_name:\n",
    "            class_name = id_to_cat_name[cat.item()] #no .item()\n",
    "        else:\n",
    "            class_name = \"Unknown\"\n",
    "        ax.text(x1, y1-5, class_name, fontsize=12, color=\"red\")\n",
    "\n",
    "    corr_boxes, corr_labels = val_batch[\"instance_bboxes\"][i], val_batch[\"instance_classes_id\"][i]\n",
    "\n",
    "    for cat, bbox in zip(corr_labels, corr_boxes):\n",
    "        x1, y1, x2, y2 = bbox\n",
    "        x1, y1, x2, y2 = (\n",
    "                x1 * img.shape[1],\n",
    "                y1 * img.shape[0],\n",
    "                x2 * img.shape[1],\n",
    "                y2 * img.shape[0],\n",
    "            )\n",
    "        rect = plt.Rectangle((x1, y1), x2-x1, y2-y1, linewidth=1, edgecolor=\"g\", facecolor=\"none\")\n",
    "        ax.add_patch(rect)\n",
    "        \n",
    "        # add label text to rect\n",
    "        if cat.item() in id_to_cat_name:\n",
    "            class_name = id_to_cat_name[cat.item()]\n",
    "        ax.text(x1, y1-5, class_name, fontsize=12, color=\"green\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Train Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove second element from first element in predicted boxes in lists of boxes labels and scores\n",
    "# remove second element from first element in predicted boxes in lists of boxes labels and scores\n",
    "\n",
    "predicted_boxes2 = [{'boxes': torch.tensor([[120.1728, 106.3066, 304.2547, 354.1632],    \n",
    "          [202.0569,   0.0000, 384.0000, 378.8736]]),\n",
    "  'labels': torch.tensor([17,  70]),\n",
    "  'scores': torch.tensor([1., 1.])},\n",
    " {'boxes': torch.tensor([[138.8928,  34.3304, 289.1942, 246.1824],\n",
    "          [142.8058, 246.2093, 223.6800, 284.7475],\n",
    "          [ 33.1200, 100.0166,  42.8160, 113.4874],\n",
    "          [351.2563, 166.5485, 384.0000, 278.6074]]),\n",
    "  'labels': torch.tensor([ 1, 41,2, 15]),\n",
    "  'scores': torch.tensor([1., 1., 1., 1.])}]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate test metrics on generated sample\n",
    "from utils.train_metrics import TrainMetrics\n",
    "device = \"cpu\"\n",
    "metrics = TrainMetrics(device=device)\n",
    "\n",
    "\n",
    "target_boxes = processor.postprocess_target_batch(val_batch, device)\n",
    "\n",
    "metrics.update(\n",
    "    predicted_boxes=predicted_boxes,\n",
    "    target_boxes=target_boxes,\n",
    "    target_texts=val_batch[\"bbox_str\"],\n",
    "    generated_text=generated_text,\n",
    ")\n",
    "metrics.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caluclate test metrics on fixed sample\n",
    "pred_boxes_test = [{\n",
    "        'boxes': torch.tensor([[183.7680,  92.0112, 332.6478, 226.5556], [169.6680,   5.6480, 324.7800, 377.7072]]), \n",
    "        'labels': torch.tensor([17, 70]), \n",
    "        'scores': torch.tensor([1., 1.])\n",
    "    }, \n",
    "    {\n",
    "        'boxes': torch.tensor([[305.1480, 223.1204, 320.6880, 235.3108], [ 73.9140,  37.1321, 219.9680, 262.9113]]),\n",
    "        'labels': torch.tensor([15,  1]), \n",
    "        'scores': torch.tensor([1., 1.])\n",
    "    }]\n",
    "target_boxes_test = [{\n",
    "        'boxes': torch.tensor([[169.6680,   5.6480, 324.7800, 377.7072], [225.7680,  92.0112, 332.6478, 226.5556]]), \n",
    "        'labels': torch.tensor([70, 17]), \n",
    "        'scores': torch.tensor([1., 1.])\n",
    "    }, \n",
    "    {\n",
    "        'boxes': torch.tensor([[305.1480, 223.1204, 320.6880, 235.3108], [ 73.9140,  37.1321, 219.9680, 262.9113]]),\n",
    "        'labels': torch.tensor([15,  1]), \n",
    "        'scores': torch.tensor([1., 1.])\n",
    "    }]\n",
    "\n",
    "test_metrics = TrainMetrics(device=device)\n",
    "test_metrics.update(predicted_boxes=pred_boxes_test, target_boxes=target_boxes_test, target_texts=val_batch[\"bbox_str\"], generated_text=generated_text)\n",
    "test_metrics.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Else"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.train_utils import build_train_dataloader\n",
    "\n",
    "train_dataloader = build_train_dataloader(config, processor)\n",
    "batch_train = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check input/labels of train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor.tokenizer.batch_decode(batch_train[\"input_ids\"], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_train[\"labels\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.all(batch_train[\"labels\"] == -100):\n",
    "    print(\"All labels are -100\")\n",
    "batch_train[\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.encode(\"<annotation>\", add_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = tokenizer.encode(\"<annotation>\", add_special_tokens=False)\n",
    "\n",
    "# find token in labels\n",
    "for i, label in enumerate(batch_train[\"labels\"]):\n",
    "    for l in label:\n",
    "        if l != -100:\n",
    "            print(l, tokenizer.decode(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_labels = batch_train[\"labels\"] != -100\n",
    "print(masked_labels.shape)\n",
    "processor.tokenizer.decode(batch_train[\"labels\"][masked_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forward_output = model.forward(\n",
    "    input_ids=batch_train[\"input_ids\"].to(device), \n",
    "    attention_mask=batch_train[\"attention_mask\"].to(device), \n",
    "    images=batch_train[\"images\"].to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask logits to only get logits for labels that are not -100\n",
    "logits_masked = forward_output.logits[masked_labels]\n",
    "print(logits_masked.shape)\n",
    "\n",
    "# check logits for first item in batch_train[\"labels\"] that is not -100\n",
    "#softmax_first_word = logits_masked.softmax(-1) #.softmax(-1) \n",
    "# plot this\n",
    "#plt.plot(softmax_first_word.detach().numpy())\n",
    "\n",
    "# get prob of annoation token as first token\n",
    "# get index of annotation token\n",
    "annotation_tag = tokenizer.encode(\"<annotation>\", add_special_tokens=False)\n",
    "print(annotation_tag)\n",
    "print(\"annotation tag logit:\", logits_masked[0][annotation_tag], \"; max logit:\", logits_masked[0].max(), \" ; max logit index:\", logits_masked[0].argmax(), \" ; max decoded:\", tokenizer.decode(logits_masked[0].argmax()))\n",
    "print(logits_masked[1][annotation_tag])\n",
    "print(logits_masked[2][annotation_tag])\n",
    "\n",
    "plt.plot(logits_masked[0].softmax(-1).detach().numpy())\n",
    "\n",
    "\n",
    "\n",
    "print(logits_masked.argmax(-1))\n",
    "tokenizer.decode(logits_masked.argmax(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_train[\"input_ids\"].shape, forward_output.logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor.tokenizer.batch_decode(forward_output.logits.argmax(dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if attention mask is correctly set\n",
    "lab = batch_train[\"labels\"]\n",
    "in_id = batch_train[\"input_ids\"]\n",
    "processor.tokenizer.decode(in_id[lab != -100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check postprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = \"<annotation><object><class>dining table</class><bbox x_min='0.0015' y_min='0.0023944' x_max='1.0' y_max='0.97319'/></object><object><class>spoon</class><bbox x_min='0.70611' y_min='0.0033803' x_max='0.92198' y_max='0.85472'/></object><object><class>cake</class><bbox x_min='0.13873' y_min='0.17587' x_max='0.72175' y_max='1.0'/></object></annotation><|im_end|>\"\n",
    "tt = \"<annotation><object><class>person</class><bbox><x27/><y67/><x29/><y71/></bbox></object><object><class>surfboard</class><bbox><x28/><y70/><x32/><y74/></bbox></object><object><class>kite</class><bbox><x75/><y09/><x90/><y36/></bbox></object><object><class>person</class><bbox><x97/><y39/><x98/><y40/></bbox></object></annotation>\"\n",
    "processor._postprocess_xml(tt, val_dataloader.dataset.dataset.cat_name_to_id, \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tt = \"<annotation><object><class>dining table</class><bbox x_min='0.0015' y_min='0.0023944' x_max='1.0' y_max='0.97319'/></object><object><class>spoon</class><bbox x_min='0.70611' y_min='0.0033803' x_max='0.92198' y_max='0.85472'/></object><object><class>cake</class><bbox x_min='0.13873' y_min='0.17587' x_max='0.72175' y_max='1.0'/></object></annotation><|im_end|>\"\n",
    "tt_token = [tokenizer.encode(tt, add_special_tokens=False)]\n",
    "processor.postprocess_xml_batch(tt_token, val_dataloader.dataset, \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try generate with new prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate with new prompt\n",
    "\n",
    "prompt = \"Detect all objects in the image and output ONLY a with specified XML tags.\"\n",
    "\n",
    "example_xml = \"<annotation><object><class>car</class><bbox x0='0.14673' y0='0.36377' x1='0.18527' y1='0.44438'/></object><object><class>surfboard</class><bbox x_min='0.0' y_min='0.41329' x_max='0.86317' y_max='0.67906'/></object></annotation>\"\n",
    "prompt = f\"Detect all objects in the image and output valid XML of root <annotation> and child <object>. Each <object> must have a <class> (string name) and <bbox (4 attributes with normalized coordinates x_min, y_min, x_max, y_max). Example: {example_xml}. Include all visible objects, even if partially visible. Output nothing but the XML.\"\n",
    "\n",
    "prompt1, gen_text = processor.prepare_text_input(config.num_image_tokens, [], [], [], prompt = prompt, train=False)\n",
    "print(prompt1)\n",
    "tokenized = tokenizer(\n",
    "            [prompt1, prompt1],\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=config.max_tokens,\n",
    "            pad_to_multiple_of=config.pad_to_multiple_of,\n",
    "            return_tensors=\"pt\",\n",
    ")\n",
    "\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids=tokenized.input_ids.to(device),\n",
    "    attention_mask=tokenized.attention_mask.to(device),\n",
    "    image=val_batch[\"images\"].to(device),\n",
    "    stopping_criteria=[JSONStoppingCriteria(processor.tokenizer)],\n",
    "    do_sample=True,\n",
    "    temperature=.6,\n",
    "    top_p = 0.9,\n",
    "    top_k = 50,\n",
    ")\n",
    "\n",
    "print(\"Output:\", processor.tokenizer.batch_decode(outputs, skip_special_tokens=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.batch_decode(outputs, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Token length plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"lmms-lab/llava-onevision-qwen2-0.5b-si\"\n",
    "config.batch_size = 1\n",
    "config.max_tokens = None\n",
    "config.pad_to_multiple_of = None\n",
    "\n",
    "processor = processor.from_config(config, add_special_tokens=None)\n",
    "model = VisionLanguageModel(config=config, image_token_index=processor.image_token_index, num_new_tokens=0, do_init=False, initializers=None)\n",
    "dataloader = build_train_dataloader(config, processor)\n",
    "\n",
    "token_sizes = []\n",
    "for batch in tqdm(dataloader, desc=\"Processing batches\"):\n",
    "    token_size = batch[\"input_ids\"].shape[1]\n",
    "    token_sizes.append(token_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate statistics\n",
    "max_size = max(token_sizes)\n",
    "min_size = min(token_sizes)\n",
    "avg_size = sum(token_sizes) / len(token_sizes)\n",
    "\n",
    "print(f\"Token size statistics:\")\n",
    "print(f\"Max: {max_size}\")\n",
    "print(f\"Min: {min_size}\")\n",
    "print(f\"Average: {avg_size:.2f}\")\n",
    "print(f\"Number of samples: {len(token_sizes)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate number of samples with token size > 3200\n",
    "count = 0\n",
    "for size in token_sizes:\n",
    "    if size > 3200:\n",
    "        count += 1\n",
    "count, count/len(token_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new plot with log scale\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.hist(token_sizes, range=(900, 6200), bins=30, orientation=\"horizontal\", log=False, color='tab:blue', edgecolor='black')\n",
    "plt.ylabel(\"Token size\", fontsize=14)\n",
    "plt.xlabel(\"Frequency\", fontsize=14)\n",
    "plt.title(\"Token size distribution\", fontsize=16)\n",
    "plt.grid(True, which=\"both\", linestyle='--', linewidth=0.5)\n",
    "\n",
    "# Get the current axes\n",
    "ax = plt.gca()\n",
    "\n",
    "# Iterate over the patches (bars) and set the color\n",
    "for patch in ax.patches:\n",
    "\tif patch.get_y() > 3200:\n",
    "\t\tpatch.set_facecolor('tab:red')\n",
    "\n",
    "# Add a legend\n",
    "import matplotlib.patches as mpatches\n",
    "blue_patch = mpatches.Patch(color='tab:blue', label='Token size <= 3200')\n",
    "red_patch = mpatches.Patch(color='tab:red', label='Token size > 3200')\n",
    "plt.legend(handles=[blue_patch, red_patch], fontsize=12)\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
